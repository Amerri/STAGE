{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import delaxes\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "class HyperG:\n",
    "    def __init__(self, H, X=None, w=None):\n",
    "        \"\"\" Initial the incident matrix, node feature matrix and hyperedge weight vector of hypergraph\n",
    "        :param H: scipy coo_matrix of shape (n_nodes, n_edges)\n",
    "        :param X: numpy array of shape (n_nodes, n_features)\n",
    "        :param w: numpy array of shape (n_edges,)\n",
    "        \"\"\"\n",
    "        assert sparse.issparse(H)\n",
    "        assert H.ndim == 2\n",
    "\n",
    "        self._H = H\n",
    "        self._n_nodes = self._H.shape[0]\n",
    "        self._n_edges = self._H.shape[1]\n",
    "\n",
    "        if X is not None:\n",
    "            assert isinstance(X, np.ndarray) and X.ndim == 2\n",
    "            self._X = X\n",
    "        else:\n",
    "            self._X = None\n",
    "\n",
    "        if w is not None:\n",
    "            self.w = w.reshape(-1)\n",
    "            assert self.w.shape[0] == self._n_edges\n",
    "        else:\n",
    "            self.w = np.ones(self._n_edges)\n",
    "\n",
    "        self._DE = None\n",
    "        self._DV = None\n",
    "        self._INVDE = None\n",
    "        self._DV2 = None\n",
    "        self._THETA = None\n",
    "        self._L = None\n",
    "\n",
    "    def num_edges(self):\n",
    "        return self._n_edges\n",
    "\n",
    "    def num_nodes(self):\n",
    "        return self._n_nodes\n",
    "\n",
    "    def incident_matrix(self):\n",
    "        return self._H\n",
    "\n",
    "    def hyperedge_weights(self):\n",
    "        return self.w\n",
    "\n",
    "    def node_features(self):\n",
    "        return self._X\n",
    "\n",
    "    def node_degrees(self):\n",
    "        if self._DV is None:\n",
    "            H = self._H.tocsr()\n",
    "            dv = H.dot(self.w.reshape(-1, 1)).reshape(-1)\n",
    "            self._DV = sparse.diags(dv, shape=(self._n_nodes, self._n_nodes))\n",
    "        return self._DV\n",
    "\n",
    "    def edge_degrees(self):\n",
    "        if self._DE is None:\n",
    "            H = self._H.tocsr()\n",
    "            de = H.sum(axis=0).A.reshape(-1)\n",
    "            self._DE = sparse.diags(de, shape=(self._n_edges, self._n_edges))\n",
    "        return self._DE\n",
    "\n",
    "    def inv_edge_degrees(self):\n",
    "        if self._INVDE is None:\n",
    "            self.edge_degrees()\n",
    "            inv_de = np.power(self._DE.data.reshape(-1), -1.)\n",
    "            self._INVDE = sparse.diags(inv_de, shape=(self._n_edges, self._n_edges))\n",
    "        return self._INVDE\n",
    "\n",
    "    def inv_square_node_degrees(self):\n",
    "        if self._DV2 is None:\n",
    "            self.node_degrees()\n",
    "            dv2 = np.power(self._DV.data.reshape(-1)+1e-6, -0.5)\n",
    "            self._DV2 = sparse.diags(dv2, shape=(self._n_nodes, self._n_nodes))\n",
    "        return self._DV2\n",
    "\n",
    "    def theta_matrix(self):\n",
    "        if self._THETA is None:\n",
    "            self.inv_square_node_degrees()\n",
    "            self.inv_edge_degrees()\n",
    "\n",
    "            W = sparse.diags(self.w)\n",
    "            self._THETA = self._DV2.dot(self._H).dot(W).dot(self._INVDE).dot(self._H.T).dot(self._DV2)\n",
    "\n",
    "        return self._THETA\n",
    "\n",
    "    def laplacian(self):\n",
    "        if self._L is None:\n",
    "            self.theta_matrix()\n",
    "            self._L = sparse.eye(self._n_nodes) - self._THETA\n",
    "        return self._L\n",
    "\n",
    "    def update_hyedge_weights(self, w):\n",
    "        assert isinstance(w, (np.ndarray, list)), \\\n",
    "            \"The hyperedge array should be a numpy.ndarray or list\"\n",
    "\n",
    "        self.w = np.array(w).reshape(-1)\n",
    "        assert w.shape[0] == self._n_edges\n",
    "\n",
    "        self._DV = None\n",
    "        self._DV2 = None\n",
    "        self._THETA = None\n",
    "        self._L = None\n",
    "\n",
    "    def update_incident_matrix(self, H):\n",
    "        assert sparse.issparse(H)\n",
    "        assert H.ndim == 2\n",
    "        assert H.shape[0] == self._n_nodes\n",
    "        assert H.shape[1] == self._n_edges\n",
    "\n",
    "        # TODO: reset hyperedge weights?\n",
    "\n",
    "        self._H = H\n",
    "        self._DE = None\n",
    "        self._DV = None\n",
    "        self._INVDE = None\n",
    "        self._DV2 = None\n",
    "        self._THETA = None\n",
    "        self._L = None\n",
    "\n",
    "def gen_attribute_hg(n_nodes, attr_dict, X=None):\n",
    "    # 构建超图集合的子图\n",
    "    \"\"\"\n",
    "    :param attr_dict: dict, eg. {'attri_1': [node_idx_1, node_idx_1, ...], 'attri_2':[...]} (zero-based indexing)\n",
    "    :param n_nodes: int,\n",
    "    :param X: numpy array, shape = (n_samples, n_features) (optional)\n",
    "    :return: instance of HyperG\n",
    "    \"\"\"\n",
    "\n",
    "    if X is not None:\n",
    "        assert n_nodes == X.shape[0]\n",
    "\n",
    "    n_edges = len(attr_dict)\n",
    "    node_idx = []\n",
    "    edge_idx = []\n",
    "\n",
    "    for idx, attr in enumerate(attr_dict):\n",
    "        nodes = sorted(attr_dict[attr])\n",
    "        node_idx.extend(nodes)\n",
    "        edge_idx.extend([idx] * len(nodes))\n",
    "\n",
    "    node_idx = np.asarray(node_idx)\n",
    "    edge_idx = np.asarray(edge_idx)\n",
    "    values = np.ones(node_idx.shape[0])\n",
    "\n",
    "    H = sparse.coo_matrix((values, (node_idx, edge_idx)), shape=(n_nodes, n_edges))\n",
    "    return HyperG(H, X=X)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_hid, n_out):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.n_hid = n_hid\n",
    "        self.n_out = n_out\n",
    "        self.linear = nn.Linear(n_hid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tx = self.linear(x)\n",
    "        return torch.log_softmax(tx.squeeze(), dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(n_hid={}, n_out={})'.format(\n",
    "            self.__class__.__name__, self.n_hid, self.n_out)\n",
    "\n",
    "\n",
    "def  set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "    # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# def construct_attr_dict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "fin = np.load('data/financial_statement.npy')\n",
    "idx = [1,0]\n",
    "\n",
    "file = 'data/mapper_dicts.json'\n",
    "with open(file,\"r\") as f: \n",
    "    maps = json.load(f)\n",
    "\n",
    "map1 = maps['mapper_idx2code']\n",
    "\n",
    "invest_cc = np.load('data/relation_invest_cc.npy')\n",
    "s1,s2 = zip(*invest_cc)\n",
    "s = set(s1).union(set((s2)))\n",
    "\n",
    "cnt = 0\n",
    "for v in s:\n",
    "    if v>=5321:\n",
    "        cnt += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 划分数据集\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch\n",
    "\n",
    "# class MyDataSet(Dataset):\n",
    "#         def __init__(self, loaded_data):\n",
    "#             self.data = loaded_data['data']\n",
    "#             self.labels = loaded_data['labels']\n",
    "    \n",
    "#         def __len__(self):\n",
    "#             return len(self.data)\n",
    "        \n",
    "#         def __getitem__(self, idx):\n",
    "#             data = self.data[idx]\n",
    "#             label = self.labels[idx]\n",
    "#             return data,label\n",
    "\n",
    "# data_mat = np.load('data/features_100.npy') #经过离散化处理\n",
    "# labels = np.load('data/risk_label.npy')\n",
    "\n",
    "# def load_data():\n",
    "\n",
    "#     num_has_fin_comp = 5321\n",
    "\n",
    "#     data1 = MyDataSet({'data':data_mat[:num_has_fin_comp],'labels':labels[:num_has_fin_comp]})\n",
    "#     data2 = MyDataSet({'data':data_mat[num_has_fin_comp:],'labels':labels[num_has_fin_comp:]})\n",
    "\n",
    "\n",
    "#     train_dataset1, validate_dataset1, test_dataset1 = torch.utils.data.random_split(data1, [0.7, 0.2, 0.1])\n",
    "#     train_dataset2, validate_dataset2, test_dataset2 = torch.utils.data.random_split(data2, [0.7, 0.2, 0.1])\n",
    "\n",
    "#     train_data1 = DataLoader(train_dataset1)\n",
    "#     train_data2 = DataLoader(train_dataset2)\n",
    "\n",
    "#     validate_data1 = DataLoader(validate_dataset1)\n",
    "#     validate_data2 = DataLoader(validate_dataset2)\n",
    "\n",
    "#     test_data1 = DataLoader(test_dataset1)\n",
    "#     test_data2 = DataLoader(test_dataset2)\n",
    "\n",
    "#     train_data_list = [iter(train_data1),iter(train_data2)]\n",
    "#     validate_data_list = [iter(validate_data1),iter(validate_data2)]\n",
    "#     test_data_list = [iter(test_data1),iter(test_data2)]\n",
    "\n",
    "#     return train_data_list, validate_data_list, test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "def load_hete_graph():\n",
    "    rel_bc2bc = np.load('data/listed_comp/relation_invest_bc2bc.npy')\n",
    "    rel_provide_bc2bc = np.load('data/listed_comp/relation_provide_bc2bc.npy')\n",
    "    rel_sale_bc2bc = np.load('data/listed_comp/relation_sale_bc2bc.npy')\n",
    "\n",
    "    feature_fin = np.load('data/listed_comp/features_norm_bc.npy')\n",
    "    feature_basic = np.load('data/listed_comp/features_basic_bc.npy')\n",
    "    # fin_feature_ttl = np.load('data/features_100.npy')\n",
    "\n",
    "    src, tgt = zip(*rel_bc2bc)\n",
    "    src_p,tgt_p = zip(*rel_provide_bc2bc)\n",
    "    src_s,tgt_s = zip(*rel_sale_bc2bc)\n",
    "\n",
    "    graph_data = { ('company','invest_bc2bc','company') : (src,tgt),\n",
    "                   ('company','provide_bc2bc','company') : (src_p,tgt_p),\n",
    "                   ('company','sale_bc2bc','company') : (src_s,tgt_s)}\n",
    "\n",
    "    g = dgl.heterograph(graph_data)\n",
    "\n",
    "    num = g.num_nodes()\n",
    "\n",
    "\n",
    "    feats_fin = th.tensor(feature_fin[:num])\n",
    "    feats_bsc = th.tensor(feature_basic[:num])\n",
    "    feats = th.cat([feats_fin,feats_bsc],1)\n",
    "    \n",
    "    g.nodes['company'].data['feature'] = feats\n",
    "\n",
    "    isnan = np.isnan(feats)\n",
    "    print(\"空值：\",True in isnan)\n",
    "\n",
    "    dict_node_feats =  {'enterprise': feats}\n",
    "\n",
    "    return g, feats, dict_node_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as D\n",
    "\n",
    "#   train_idx = train_data.indices\n",
    "def split_data():\n",
    "    g, feats, dict_node_features = load_hete_graph()\n",
    "    labels_ttl = np.load('data/risk_label.npy')\n",
    "    num_nodes = g.num_nodes()\n",
    "    labels = th.tensor(labels_ttl[:num_nodes])\n",
    "    train_size = int(num_nodes * 0.6)\n",
    "    val_size = int(num_nodes * 0.2)\n",
    "    test_size = num_nodes - train_size - val_size\n",
    "    train_data, val_data, test_data = D.random_split(feats,[train_size,val_size,test_size])\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict形式： { 'industry': {'K01':[0,1,2], ... } }\n",
    "import json\n",
    "\n",
    "def load_hyper_graph():\n",
    "    filename = 'data/dicts_hyper.json'\n",
    "    with open(filename,\"r\") as f: \n",
    "        hyper_graph = json.load(f)\n",
    "    return hyper_graph\n",
    "\n",
    "# def load_train_hyper_graph(train_data):\n",
    "#     hyper_graph = load_hyper_graph()\n",
    "#     train_idx = train_data.indices\n",
    "#     dicts_industry = hyper_graph['industry']\n",
    "#     dicts_train = { _key :[] for _key in dicts_industry}\n",
    "#     for idx in train_idx:\n",
    "#         for key in dicts_industry:\n",
    "#             value = dicts_industry[key]\n",
    "#             if idx in value:\n",
    "#                 dicts_train[key].append(idx)\n",
    "#     return dicts_train\n",
    "\n",
    "def load_sub_hyper_graph(hyper_graph_data): # hyper_graph_data : dict\n",
    "    hyper_graph = load_hyper_graph()\n",
    "    train_idx = hyper_graph_data.indices\n",
    "    dicts_industry = hyper_graph['industry']\n",
    "    dicts_sub_hyper_graph = { _key :[] for _key in dicts_industry}\n",
    "    for idx in train_idx:\n",
    "        for key in dicts_industry:\n",
    "            value = dicts_industry[key]\n",
    "            if idx in value:\n",
    "                dicts_sub_hyper_graph[key].append(idx)\n",
    "    return dicts_sub_hyper_graph\n",
    "\n",
    "# def get_sub_hete_graph(train_data):\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "train_data, val_data,test_data = split_data()\n",
    "\n",
    "sub_hyp_graph = load_sub_hyper_graph(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_hete_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_428/172370009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_node_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hete_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_hete_graph' is not defined"
     ]
    }
   ],
   "source": [
    "g, feats, dict_node_feats = load_hete_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# relation1 = g.edge_type_subgraph([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "def sparse_to_adjlist(sp_matrix):\n",
    "\t\"\"\"\n",
    "\tTransfer sparse matrix to adjacency list\n",
    "\t:param sp_matrix: the sparse matrix\n",
    "\t:param filename: the filename of adjlist\n",
    "\t\"\"\"\n",
    "\t# add self loop\n",
    "\tprint(sp_matrix.shape[0])\n",
    "\thomo_adj = sp_matrix+ sp.eye(sp_matrix.shape[0])\n",
    "\t# homo_adj = sp.eye(sp_matrix.shape[0])\n",
    "\n",
    "\t# create adj_list\n",
    "\tadj_lists = defaultdict(set)\n",
    "\tedges = homo_adj.nonzero()\n",
    "\tfor index, node in enumerate(edges[0]):\n",
    "\t\tadj_lists[node].add(edges[1][index])\n",
    "\t\tadj_lists[edges[1][index]].add(node)\n",
    "\n",
    "\treturn adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_sp_to_adlist(adj1):\n",
    "    indices1 = np.array(adj1.indices())\n",
    "    values1  = np.array(adj1.val)\n",
    "    aj1 = sp.coo_matrix((values1,(indices1[0],indices1[1])),shape=[5317,5317])\n",
    "    ajlist1 =  sparse_to_adjlist(aj1)\n",
    "    return ajlist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5317\n",
      "5317\n",
      "5317\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "g = dgl.load_graphs('data/lst_comps.dgl')[0][0]\n",
    "feat_data = g.ndata['feature']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "relation1 = g.edge_type_subgraph(['invest_bc2bc'])\n",
    "relation2 = g.edge_type_subgraph(['provide_bc2bc'])\n",
    "relation3 = g.edge_type_subgraph(['sale_bc2bc'])\n",
    "\n",
    "adj1 = relation1.adjacency_matrix()\n",
    "adj2 = relation2.adjacency_matrix()\n",
    "adj3 = relation3.adjacency_matrix()\n",
    "\n",
    "# indices1 = np.array(adj1.indices())\n",
    "# values1  = np.array(adj1.val)\n",
    "# aj1 = sp.coo_matrix((values1,(indices1[0],indices1[1])),shape=[5317,5317])\n",
    "# ajlist1 =  sparse_to_adjlist(aj1)\n",
    "\n",
    "# indices2 = np.array(adj2.indices())\n",
    "# values2  = np.array(adj2.val)\n",
    "# aj2 = sp.coo_matrix((values2,(indices2[0],indices2[1])),shape=[5317,5317])\n",
    "# ajlist2 =  sparse_to_adjlist(aj2)\n",
    "\n",
    "ajlist1 = converse_sp_to_adlist(adj1)\n",
    "ajlist2 = converse_sp_to_adlist(adj2)\n",
    "ajlist3 = converse_sp_to_adlist(adj3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欺诈样本数量：559\n",
      "样本总数量：5317\n",
      "欺诈样本比例：0.10513447432762836\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "g = dgl.load_graphs('data/lst_comps.dgl')[0][0]\n",
    "feat_data = g.ndata['feature']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "print(f'欺诈样本数量：{len(labels[labels==1])}')\n",
    "print(f'样本总数量：{len(labels)}')\n",
    "print(f'欺诈样本比例：{len(labels[labels==1]) / len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
