{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM模型\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from sklearn.metrics import classification_report # 结果评估\n",
    "\n",
    "data_mat = np.load('data/features_100.npy') #经过离散化处理\n",
    "labels = np.load('data/risk_label.npy')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_mat[:5321], labels[:5321], test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear',probability=True, random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_score = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6927    0.9277    0.7932       678\n",
      "           1     0.6879    0.2791    0.3971       387\n",
      "\n",
      "    accuracy                         0.6920      1065\n",
      "   macro avg     0.6903    0.6034    0.5951      1065\n",
      "weighted avg     0.6910    0.6920    0.6492      1065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "        def __init__(self, loaded_data):\n",
    "            self.data = loaded_data['data']\n",
    "            self.labels = loaded_data['labels']\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            data = self.data[idx]\n",
    "            label = self.labels[idx]\n",
    "            return data,label\n",
    "\n",
    "custom_data = MyDataSet({'data':data_mat,'labels':labels})\n",
    "\n",
    "\n",
    "train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(custom_data, [0.5, 0.3, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_dataset,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypergnn + hetegnn\n",
    "\n",
    "from utils import *\n",
    "from models import HyperSTGNN\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import recall_score as rec\n",
    "from sklearn.metrics import precision_score as pre\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "\n",
    "\n",
    "\n",
    "n_epoch = 200\n",
    "clip = 0.25\n",
    "\n",
    "g, features, dict_node_feats = load_hete_graph()\n",
    "\n",
    "labels_ttl = np.load('data/risk_label.npy')\n",
    "num_nodes = g.num_nodes()\n",
    "labels = torch.tensor(labels_ttl[:num_nodes])\n",
    "\n",
    "input_dim = 49\n",
    "output_dim = 20\n",
    "total_company_num = g.num_nodes()\n",
    "rel_num = 1\n",
    "com_initial_emb = features\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 =0 \n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "# set_random_seed(14)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# todo:\n",
    "train_data,val_data,test_data = split_data()\n",
    "\n",
    "train_idx = train_data.indices\n",
    "valid_idx = val_data.indices\n",
    "\n",
    "train_hyp_graph = load_sub_hyper_graph(train_data)\n",
    "val_hyp_graph = load_sub_hyper_graph(val_data)\n",
    "test_hyp_graph = load_sub_hyper_graph(test_data)\n",
    "\n",
    "def train():\n",
    "    \n",
    "    gnn = HyperSTGNN(input_dim,output_dim,\n",
    "                     total_company_num,rel_num,\n",
    "                     device,com_initial_emb,g,dict_node_feats,\n",
    "                     num_heads=1,dropout=0.2,norm=True)\n",
    "\n",
    "    classifier = Classifier(output_dim, 2).to(device)\n",
    "    model = nn.Sequential(gnn, classifier)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 20, eta_min=1e-6)\n",
    "\n",
    "    ['industry', 'area', 'qualify']\n",
    "    train_hyp=[]\n",
    "    for i in ['industry']:\n",
    "        train_hyp+=[gen_attribute_hg(total_company_num, train_hyp_graph[i], X=None)]\n",
    "    valid_hyp=[]\n",
    "    for i in ['industry']:\n",
    "        valid_hyp+=[gen_attribute_hg(total_company_num, val_hyp_graph[i], X=None)]\n",
    "    test_hyp=[]\n",
    "    for i in ['industry']:\n",
    "        test_hyp+=[gen_attribute_hg(total_company_num, test_hyp_graph[i], X=None)]\n",
    "\n",
    "\n",
    "    for epoch in np.arange(n_epoch):\n",
    "\n",
    "        st=time.time()\n",
    "\n",
    "        '''\n",
    "            Train \n",
    "        '''\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        # TODO \n",
    "        company_emb=gnn.forward(g,dict_node_feats,train_hyp,train_idx)\n",
    "\n",
    "        res = classifier.forward(company_emb)\n",
    "\n",
    "        loss = criterion(res, torch.LongTensor(train_label))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses += [loss.cpu().detach().tolist()]\n",
    "        # train_step += 1\n",
    "        scheduler.step()\n",
    "        del res, loss\n",
    "\n",
    "        '''\n",
    "            Valid \n",
    "        '''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            company_emb=gnn.forward(g,dict_node_feats,valid_hyp,valid_idx)\n",
    "\n",
    "            res = classifier.forward(company_emb)\n",
    "            valid_label = labels[valid_idx]\n",
    "            loss = criterion(res,torch.LongTensor(valid_label) )\n",
    "\n",
    "            pred=res.argmax(dim=1)\n",
    "            ac=acc(valid_label,pred)\n",
    "            pr=pre(valid_label,pred)\n",
    "            re=rec(valid_label,pred)\n",
    "            f=f1(valid_label,pred)\n",
    "            rc=roc(valid_label,res[:,1])\n",
    "\n",
    "            if ac > best_acc and f>best_f1:\n",
    "                best_acc = ac\n",
    "                best_f1=f\n",
    "                torch.save(model, './model_save/%s.pkl'%('best_model'))\n",
    "\n",
    "                print('UPDATE!!!')\n",
    "\n",
    "\n",
    "            et = time.time()\n",
    "            print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid Acc: %.4f Valid Pre: %.4f  Valid Recall: %.4f Valid F1: %.4f  Valid Roc: %.4f\"  ) % \\\n",
    "                (epoch, (et - st), optimizer.param_groups[0]['lr'], np.average(train_losses), \\\n",
    "                loss.cpu().detach().tolist(), ac,pr,re,f,rc))\n",
    "\n",
    "            del res, loss\n",
    "\n",
    "            if epoch+1==n_epoch:\n",
    "                company_emb=gnn.forward(g,dict_node_feats,test_hyp,test_idx)\n",
    "                # gnn.forward(g,dict_node_feats,valid_hyp,valid_idx)\n",
    "                test_label = labels[test_idx]\n",
    "                res = classifier.forward(company_emb)\n",
    "\n",
    "                pred=res.argmax(dim=1)\n",
    "                ac=acc(test_label,pred)\n",
    "                pr=pre(test_label,pred)\n",
    "                re=rec(test_label,pred)\n",
    "                f=f1(test_label,pred)\n",
    "                rc=roc(test_label,res[:,1])\n",
    "                \n",
    "                print('Last Test Acc: %.4f Last Test Pre: %.4f Last Test Recall: %.4f Last Test F1: %.4f Last Test ROC: %.4f' % (ac,pr,re,f,rc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data():\n",
    "    g, feats, dict_node_features = load_hete_graph()\n",
    "    labels_ttl = np.load('data/risk_label.npy')\n",
    "    num_nodes = g.num_nodes()\n",
    "    labels = torch.tensor(labels_ttl[:num_nodes])\n",
    "    train_size = int(num_nodes * 0.6)\n",
    "    val_size = int(num_nodes * 0.2)\n",
    "    test_size = num_nodes - train_size - val_size\n",
    "    train_data, val_data, test_data = torch.utils.data.random_split(feats,[train_size,val_size,test_size])\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data,val_data,test_data = split_data()\n",
    "\n",
    "def load_sub_hyper_graph(hyper_graph_data): # hyper_graph_data : dict\n",
    "    hyper_graph = load_hyper_graph()\n",
    "    train_idx = hyper_graph_data.indices\n",
    "    dicts_industry = hyper_graph['industry']\n",
    "    dicts_sub_hyper_graph = { _key :[] for _key in dicts_industry}\n",
    "    for idx in train_idx:\n",
    "        for key in dicts_industry:\n",
    "            value = dicts_industry[key]\n",
    "            if idx in value:\n",
    "                dicts_sub_hyper_graph[key].append(idx)\n",
    "    return dicts_sub_hyper_graph\n",
    "\n",
    "train_idx = train_data.indices\n",
    "\n",
    "train_hyp_graph = load_sub_hyper_graph(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier(max_features='sqrt', random_state=1937093881), DecisionTreeClassifier(max_features='sqrt', random_state=558624822), DecisionTreeClassifier(max_features='sqrt', random_state=1651469672), DecisionTreeClassifier(max_features='sqrt', random_state=30501663), DecisionTreeClassifier(max_features='sqrt', random_state=961235350), DecisionTreeClassifier(max_features='sqrt', random_state=1329811137), DecisionTreeClassifier(max_features='sqrt', random_state=2047088086), DecisionTreeClassifier(max_features='sqrt', random_state=1550603074), DecisionTreeClassifier(max_features='sqrt', random_state=1805770369), DecisionTreeClassifier(max_features='sqrt', random_state=1635701129), DecisionTreeClassifier(max_features='sqrt', random_state=1584289669), DecisionTreeClassifier(max_features='sqrt', random_state=1859110834), DecisionTreeClassifier(max_features='sqrt', random_state=1042605351), DecisionTreeClassifier(max_features='sqrt', random_state=1359248650), DecisionTreeClassifier(max_features='sqrt', random_state=1169048507), DecisionTreeClassifier(max_features='sqrt', random_state=1783148097), DecisionTreeClassifier(max_features='sqrt', random_state=786963957), DecisionTreeClassifier(max_features='sqrt', random_state=628873511), DecisionTreeClassifier(max_features='sqrt', random_state=1429160085), DecisionTreeClassifier(max_features='sqrt', random_state=1850060443), DecisionTreeClassifier(max_features='sqrt', random_state=1500247834), DecisionTreeClassifier(max_features='sqrt', random_state=953989420), DecisionTreeClassifier(max_features='sqrt', random_state=232364320), DecisionTreeClassifier(max_features='sqrt', random_state=378536957), DecisionTreeClassifier(max_features='sqrt', random_state=86453985), DecisionTreeClassifier(max_features='sqrt', random_state=1742865264), DecisionTreeClassifier(max_features='sqrt', random_state=1571197671), DecisionTreeClassifier(max_features='sqrt', random_state=697718179), DecisionTreeClassifier(max_features='sqrt', random_state=55172579), DecisionTreeClassifier(max_features='sqrt', random_state=1517886542), DecisionTreeClassifier(max_features='sqrt', random_state=358863669), DecisionTreeClassifier(max_features='sqrt', random_state=433686411), DecisionTreeClassifier(max_features='sqrt', random_state=921868188), DecisionTreeClassifier(max_features='sqrt', random_state=867033264), DecisionTreeClassifier(max_features='sqrt', random_state=1739812951), DecisionTreeClassifier(max_features='sqrt', random_state=848772500), DecisionTreeClassifier(max_features='sqrt', random_state=948646838), DecisionTreeClassifier(max_features='sqrt', random_state=263512463), DecisionTreeClassifier(max_features='sqrt', random_state=1540174716), DecisionTreeClassifier(max_features='sqrt', random_state=730680396), DecisionTreeClassifier(max_features='sqrt', random_state=2064641714), DecisionTreeClassifier(max_features='sqrt', random_state=438722674), DecisionTreeClassifier(max_features='sqrt', random_state=720540847), DecisionTreeClassifier(max_features='sqrt', random_state=953525449), DecisionTreeClassifier(max_features='sqrt', random_state=38897330), DecisionTreeClassifier(max_features='sqrt', random_state=1453197563), DecisionTreeClassifier(max_features='sqrt', random_state=1553364963), DecisionTreeClassifier(max_features='sqrt', random_state=683699217), DecisionTreeClassifier(max_features='sqrt', random_state=1872815896), DecisionTreeClassifier(max_features='sqrt', random_state=1914398587), DecisionTreeClassifier(max_features='sqrt', random_state=1286500971), DecisionTreeClassifier(max_features='sqrt', random_state=101512971), DecisionTreeClassifier(max_features='sqrt', random_state=7004014), DecisionTreeClassifier(max_features='sqrt', random_state=1167623322), DecisionTreeClassifier(max_features='sqrt', random_state=108267754), DecisionTreeClassifier(max_features='sqrt', random_state=629619272), DecisionTreeClassifier(max_features='sqrt', random_state=2071742330), DecisionTreeClassifier(max_features='sqrt', random_state=28757471), DecisionTreeClassifier(max_features='sqrt', random_state=869449033), DecisionTreeClassifier(max_features='sqrt', random_state=1529278233), DecisionTreeClassifier(max_features='sqrt', random_state=1211974480), DecisionTreeClassifier(max_features='sqrt', random_state=901889548), DecisionTreeClassifier(max_features='sqrt', random_state=1190285585), DecisionTreeClassifier(max_features='sqrt', random_state=888172462), DecisionTreeClassifier(max_features='sqrt', random_state=598310146), DecisionTreeClassifier(max_features='sqrt', random_state=1614924769), DecisionTreeClassifier(max_features='sqrt', random_state=1938443324), DecisionTreeClassifier(max_features='sqrt', random_state=697448764), DecisionTreeClassifier(max_features='sqrt', random_state=473531580), DecisionTreeClassifier(max_features='sqrt', random_state=937918411), DecisionTreeClassifier(max_features='sqrt', random_state=479852634), DecisionTreeClassifier(max_features='sqrt', random_state=1515633492), DecisionTreeClassifier(max_features='sqrt', random_state=1953621600), DecisionTreeClassifier(max_features='sqrt', random_state=1428837839), DecisionTreeClassifier(max_features='sqrt', random_state=986939312), DecisionTreeClassifier(max_features='sqrt', random_state=491059232), DecisionTreeClassifier(max_features='sqrt', random_state=951767043), DecisionTreeClassifier(max_features='sqrt', random_state=1388616406), DecisionTreeClassifier(max_features='sqrt', random_state=1919521128), DecisionTreeClassifier(max_features='sqrt', random_state=1159020246), DecisionTreeClassifier(max_features='sqrt', random_state=1658895273), DecisionTreeClassifier(max_features='sqrt', random_state=309957587), DecisionTreeClassifier(max_features='sqrt', random_state=1130305076), DecisionTreeClassifier(max_features='sqrt', random_state=1252149698), DecisionTreeClassifier(max_features='sqrt', random_state=659497768), DecisionTreeClassifier(max_features='sqrt', random_state=76406480), DecisionTreeClassifier(max_features='sqrt', random_state=1513854181), DecisionTreeClassifier(max_features='sqrt', random_state=1277159488), DecisionTreeClassifier(max_features='sqrt', random_state=399949192), DecisionTreeClassifier(max_features='sqrt', random_state=43892878), DecisionTreeClassifier(max_features='sqrt', random_state=2022064017), DecisionTreeClassifier(max_features='sqrt', random_state=1534014804), DecisionTreeClassifier(max_features='sqrt', random_state=1276502181), DecisionTreeClassifier(max_features='sqrt', random_state=1324910984), DecisionTreeClassifier(max_features='sqrt', random_state=552207581), DecisionTreeClassifier(max_features='sqrt', random_state=224937289), DecisionTreeClassifier(max_features='sqrt', random_state=1865611220), DecisionTreeClassifier(max_features='sqrt', random_state=1705082775), DecisionTreeClassifier(max_features='sqrt', random_state=388698928), DecisionTreeClassifier(max_features='sqrt', random_state=1401526229)]\n",
      "Feature Importances: [4.15063359e-02 1.36346063e-02 2.85239052e-03 1.69077016e-02\n",
      " 2.21710038e-02 1.56820960e-02 2.39410250e-02 3.97205054e-03\n",
      " 1.02906213e-02 3.06128632e-02 2.15833063e-02 8.08380938e-03\n",
      " 5.41885925e-03 2.27697882e-02 2.63165323e-02 5.33888385e-03\n",
      " 2.98976011e-02 1.41696375e-02 1.12117687e-02 1.36658492e-02\n",
      " 2.48258107e-02 2.31520229e-02 1.96732414e-02 2.77328195e-02\n",
      " 2.20093794e-02 1.75890509e-02 1.64444919e-02 2.23502560e-02\n",
      " 7.59601471e-03 2.15622086e-02 3.42184738e-02 2.64219499e-02\n",
      " 5.86195345e-03 5.57249348e-03 3.59973761e-02 1.86233712e-02\n",
      " 1.84643540e-02 2.58014062e-02 1.25712793e-02 6.63383256e-03\n",
      " 1.66363094e-02 2.35215302e-02 1.73546405e-03 2.34418956e-02\n",
      " 1.57625625e-03 6.75614111e-02 3.06191665e-02 2.00722127e-02\n",
      " 2.70579941e-02 2.05669622e-02 2.58542416e-02 2.34198131e-03\n",
      " 1.94647313e-05 0.00000000e+00 0.00000000e+00 3.16376321e-03\n",
      " 2.70283102e-03]\n",
      "[54 53 52 44 42 51 56  2 55  7 15 12 33 32 39 28 11  8 18 38  1 19 17  5\n",
      " 26 40  3 25 36 35 22 47 49 29 10 24  4 27 13 21 43 41  6 20 37 50 14 31\n",
      " 48 23 16  9 46 30 34  0 45]\n"
     ]
    }
   ],
   "source": [
    "# 随机森林 特征值重要性 评估\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import dgl as dl\n",
    "\n",
    "def feature_importance(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    print(model.estimators_)\n",
    "    importances = model.feature_importances_\n",
    "    return importances\n",
    "\n",
    "# 数据\n",
    "g = dl.load_graphs('data/lst_comps7.dgl')[0][0]\n",
    "feats = g.nodes['company'].data['feature'] \n",
    "labels = np.load('data/listed_comp/labels_listed_comp.npy')\n",
    "\n",
    "# 计算特征重要性\n",
    "importances = feature_importance(feats, labels)\n",
    "print(\"Feature Importances:\", importances)\n",
    "feats_sort_idx = np.argsort(importances)\n",
    "print(feats_sort_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5317, 26])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.02\n",
    "x_selected = feats[:, importances > threshold]\n",
    "x_selected.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
